{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming  \n",
    "In this task, you will implement Spark Structured Streaming to consume the data from task 1 and perform a prediction.    \n",
    "Important:   \n",
    "-\tThis task uses PySpark Structured Streaming with PySpark Dataframe APIs and PySpark ML.  \n",
    "-\tYou also need your pipeline model from A2A to make predictions and persist the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession created successfully\n",
      "Application Name: FIT5202_A2B_WeatherStream\n",
      "Master: local[4]\n",
      "Timezone: Australia/Melbourne\n",
      "Checkpoint Directory: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- CONFIGURATION ----------\n",
    "APP_NAME = \"FIT5202_A2B_WeatherStream\"\n",
    "CHECKPOINT_DIR = \"/home/student/checkpoints/a2b_task1\"  # make sure this exists or will be created\n",
    "TIMEZONE = \"Australia/Melbourne\"\n",
    "\n",
    "# ---------- CREATE SPARK SESSION ----------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_VER = \"3.5.0\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(APP_NAME)\n",
    "    .master(\"local[4]\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\n",
    "    .config(\"spark.driver.memory\", \"7g\")\n",
    "    .config(\"spark.executor.memory\", \"7g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .config(\"spark.memory.fraction\", \"0.6\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{SPARK_VER},\"\n",
    "        f\"org.apache.kafka:kafka-clients:{SPARK_VER}\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"64m\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "\n",
    "print(\"✅ SparkSession created successfully\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Timezone: {spark.conf.get('spark.sql.session.timeZone')}\")\n",
    "print(f\"Checkpoint Directory: {spark.conf.get('spark.sql.streaming.checkpointLocation')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWrite code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. building information) into data frames. (You can reuse your code from 2A.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Meters:\n",
      "root\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- meter_type: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- value: decimal(20,6) (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      "\n",
      "Sample Meters:\n",
      "+-----------+----------+-------------------+---------+------+\n",
      "|building_id|meter_type|ts                 |value    |row_id|\n",
      "+-----------+----------+-------------------+---------+------+\n",
      "|163        |c         |2022-01-01 00:00:00|4.571900 |3     |\n",
      "|170        |c         |2022-01-01 00:00:00|11.289100|8     |\n",
      "|171        |c         |2022-01-01 00:00:00|0.000000 |9     |\n",
      "|172        |c         |2022-01-01 00:00:00|0.000000 |10    |\n",
      "|174        |c         |2022-01-01 00:00:00|52.858300|12    |\n",
      "+-----------+----------+-------------------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === A2B — Define schemas per metadata & load static datasets (reusing 2A style) ===\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, TimestampType, DecimalType, DoubleType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------------------------\n",
    "# 1) meters.csv schema (time-series) — per metadata\n",
    "# ---------------------------\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(),  False),\n",
    "    StructField(\"meter_type\",  StringType(),   False),   # Char(1) -> StringType in Spark\n",
    "    StructField(\"ts\",          TimestampType(),False),\n",
    "    StructField(\"value\",       DecimalType(20, 6), False),\n",
    "    StructField(\"row_id\",      IntegerType(),  False),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Paths (adjust to your files)\n",
    "# ---------------------------\n",
    "METERS_CSV    = \"new_meters.csv\"              \n",
    "\n",
    "# Load meters/weather as static tables (only if needed now)\n",
    "# ---------------------------\n",
    "meters_df = (spark.read\n",
    "     .option(\"header\", True)\n",
    "      .schema(meters_schema)\n",
    "     .csv(METERS_CSV))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Meters:\")\n",
    "meters_df.printSchema()\n",
    "print(\"Sample Meters:\")\n",
    "meters_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUsing the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'weather_ts' column, you shall receive it as an Int type. Load the new building information CSV file into a dataframe. Then, the data frames should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark_app_weather_stream.py  (no building join)\n",
    "\n",
    "import time, uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, array, coalesce, explode_outer,\n",
    "    to_timestamp, from_unixtime\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    ")\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "TIMEZONE                 = \"Australia/Melbourne\"\n",
    "CHECKPOINT_DIR  = \"/home/student/checkpoints/a2b_task1/weather\"\n",
    "OUT_WEATHER     = \"/home/student/out/weather_6h_ts\"\n",
    "KAFKA_BOOTSTRAP          = \"kafka:9092\"\n",
    "TOPIC_SUBSCRIBE_PATTERN  = r\"weather-site-\\d+\"\n",
    "STARTING_OFFSETS         = \"latest\"   # use \"earliest\" if you want to replay retained data\n",
    "\n",
    "# ------------------ KAFKA SOURCE -----------------\n",
    "kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribePattern\", TOPIC_SUBSCRIBE_PATTERN)\n",
    "    .option(\"startingOffsets\", STARTING_OFFSETS)\n",
    "    .option(\"failOnDataLoss\", \"false\")  # dev-friendly: don’t crash if retention trimmed\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# value is binary; cast to string\n",
    "value_str_df = kafka_df.selectExpr(\n",
    "    \"CAST(value AS STRING) AS value\", \"topic\", \"partition\", \"offset\", \"timestamp as kafka_ingest_ts\"\n",
    ")\n",
    "\n",
    "# ------------------ WIRE SCHEMA ------------------\n",
    "wire_obj_schema = StructType([\n",
    "    StructField(\"site_id\",            StringType(),  True),\n",
    "    StructField(\"timestamp\",          StringType(),  True),\n",
    "    StructField(\"air_temperature\",    StringType(),  True),\n",
    "    StructField(\"cloud_coverage\",     StringType(),  True),\n",
    "    StructField(\"dew_temperature\",    StringType(),  True),\n",
    "    StructField(\"sea_level_pressure\", StringType(),  True),\n",
    "    StructField(\"wind_direction\",     StringType(),  True),\n",
    "    StructField(\"wind_speed\",         StringType(),  True),\n",
    "    StructField(\"weather_ts\",         IntegerType(), True),\n",
    "    StructField(\"day_index\",          StringType(),  True),\n",
    "])\n",
    "\n",
    "wire_array_schema = ArrayType(wire_obj_schema)\n",
    "\n",
    "parsed_arr = value_str_df.select(\n",
    "    col(\"value\"),\n",
    "    col(\"topic\"), col(\"partition\"), col(\"offset\"), col(\"kafka_ingest_ts\"),\n",
    "    from_json(col(\"value\"), wire_array_schema).alias(\"rows_arr\"),\n",
    "    from_json(col(\"value\"), wire_obj_schema).alias(\"row_obj\")\n",
    ")\n",
    "\n",
    "rows_df = parsed_arr.select(\n",
    "    col(\"topic\"), col(\"partition\"), col(\"offset\"), col(\"kafka_ingest_ts\"),\n",
    "    coalesce(col(\"rows_arr\"), array(col(\"row_obj\"))).alias(\"rows\")\n",
    ")\n",
    "\n",
    "exploded = rows_df.select(\n",
    "    col(\"topic\"), col(\"partition\"), col(\"offset\"), col(\"kafka_ingest_ts\"),\n",
    "    explode_outer(col(\"rows\")).alias(\"r\")\n",
    ").select(\n",
    "    \"topic\", \"partition\", \"offset\", \"kafka_ingest_ts\",\n",
    "    col(\"r.site_id\").alias(\"site_id_str\"),\n",
    "    col(\"r.timestamp\").alias(\"ts_str\"),\n",
    "    col(\"r.air_temperature\").alias(\"air_temperature_str\"),\n",
    "    col(\"r.cloud_coverage\").alias(\"cloud_coverage_str\"),\n",
    "    col(\"r.dew_temperature\").alias(\"dew_temperature_str\"),\n",
    "    col(\"r.sea_level_pressure\").alias(\"sea_level_pressure_str\"),\n",
    "    col(\"r.wind_direction\").alias(\"wind_direction_str\"),\n",
    "    col(\"r.wind_speed\").alias(\"wind_speed_str\"),\n",
    "    col(\"r.weather_ts\").alias(\"weather_ts_int\"),\n",
    "    col(\"r.day_index\").alias(\"day_index_str\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildings schema:\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- primary_use: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- floor_count: integer (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      " |-- latent_y: decimal(20,6) (nullable = true)\n",
      " |-- latent_s: decimal(20,6) (nullable = true)\n",
      " |-- latent_r: decimal(20,6) (nullable = true)\n",
      "\n",
      "Sample buildings:\n",
      "+-------+-----------+-----------+-----------+-----------+------+----------+---------+--------+--------+\n",
      "|site_id|building_id|primary_use|square_feet|floor_count|row_id|year_built|latent_y |latent_s|latent_r|\n",
      "+-------+-----------+-----------+-----------+-----------+------+----------+---------+--------+--------+\n",
      "|10     |1017       |Technology |109263     |6          |1018  |1971      |29.000000|4.260310|4.000000|\n",
      "|4      |587        |Technology |53234      |5          |588   |1949      |51.000000|4.027186|3.000000|\n",
      "|13     |1081       |Industrial |11470      |1          |1082  |2005      |5.000000 |4.059564|4.000000|\n",
      "|13     |1171       |Industrial |56851      |1          |1172  |2022      |22.000000|4.754738|4.000000|\n",
      "|15     |1340       |Industrial |111518     |1          |1341  |1975      |25.000000|5.047345|4.000000|\n",
      "+-------+-----------+-----------+-----------+-----------+------+----------+---------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2) buildings.csv schema (static) — per metadata\n",
    "# ---------------------------\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\",       IntegerType(),      False),\n",
    "    StructField(\"building_id\",   IntegerType(),      False),\n",
    "    StructField(\"primary_use\",   StringType(),       True),\n",
    "    StructField(\"square_feet\",   IntegerType(),      True),\n",
    "    StructField(\"floor_count\",   IntegerType(),      True),\n",
    "    StructField(\"row_id\",        IntegerType(),      False),\n",
    "    StructField(\"year_built\",    IntegerType(),      True),\n",
    "    StructField(\"latent_y\",      DecimalType(20, 6), True),\n",
    "    StructField(\"latent_s\",      DecimalType(20, 6), True),\n",
    "    StructField(\"latent_r\",      DecimalType(20, 6), True),\n",
    "])\n",
    "BUILDINGS_CSV = \"new_building_information.csv\"   \n",
    "\n",
    "# ---------------------------\n",
    "# Load STATIC dataset(s): buildings.csv\n",
    "# ---------------------------\n",
    "buildings_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .schema(buildings_schema)\n",
    "         .csv(BUILDINGS_CSV)\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Quick sanity prints\n",
    "# ---------------------------\n",
    "print(\"Buildings schema:\")\n",
    "buildings_df.printSchema()\n",
    "print(\"Sample buildings:\")\n",
    "buildings_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse a watermark on weather_ts, if data points are received 5 seconds late, discard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Make sure session TZ matches producer/CSV expectations\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\n",
    "\n",
    "# 1) Parse columns\n",
    "weather_base = (\n",
    "    exploded\n",
    "    .withColumn(\"measure_ts\", F.to_timestamp(\"ts_str\"))                # 2022 event time\n",
    "    .withColumn(\"event_time\", F.to_timestamp(F.from_unixtime(\"weather_ts_int\")))  # producer clock\n",
    "    .withColumn(\"site_id\", F.col(\"site_id_str\").cast(\"int\"))\n",
    "    .withColumn(\"air_temperature\",    F.col(\"air_temperature_str\").cast(\"double\"))\n",
    "    .withColumn(\"cloud_coverage\",     F.col(\"cloud_coverage_str\").cast(\"double\"))\n",
    "    .withColumn(\"dew_temperature\",    F.col(\"dew_temperature_str\").cast(\"double\"))\n",
    "    .withColumn(\"sea_level_pressure\", F.col(\"sea_level_pressure_str\").cast(\"double\"))\n",
    "    .withColumn(\"wind_direction\",     F.col(\"wind_direction_str\").cast(\"int\"))\n",
    "    .withColumn(\"wind_speed\",         F.col(\"wind_speed_str\").cast(\"double\"))\n",
    "    .drop(\n",
    "        \"site_id_str\",\"ts_str\",\"air_temperature_str\",\"cloud_coverage_str\",\n",
    "        \"dew_temperature_str\",\"sea_level_pressure_str\",\"wind_direction_str\",\"wind_speed_str\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Watermark on event_time (assignment) + dedup at hour grain\n",
    "dedupbed = (\n",
    "    weather_base\n",
    "    .withWatermark(\"event_time\", \"5 seconds\")\n",
    "    .withColumn(\"measure_hour\", F.date_trunc(\"hour\", F.col(\"measure_ts\")))\n",
    "    .dropDuplicates([\"site_id\", \"measure_hour\"])   # bounded state ~140k keys in your scale\n",
    ")\n",
    "\n",
    "# 3) Derive join keys & friendly label\n",
    "with_keys = (\n",
    "    dedupbed\n",
    "    .withColumn(\"date\", F.to_date(\"measure_hour\"))\n",
    "    .withColumn(\"slot\", F.floor(F.hour(\"measure_hour\")/6).cast(\"int\"))\n",
    "    .withColumn(\n",
    "        \"slot_label\",\n",
    "        F.when(F.col(\"slot\")==0, F.lit(\"00:00-05:59\"))\n",
    "         .when(F.col(\"slot\")==1, F.lit(\"06:00-11:59\"))\n",
    "         .when(F.col(\"slot\")==2, F.lit(\"12:00-17:59\"))\n",
    "         .otherwise(F.lit(\"18:00-23:59\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) 6h aggregates\n",
    "weather_6h = (\n",
    "    with_keys\n",
    "    .groupBy(\"site_id\",\"date\",\"slot\",\"slot_label\")\n",
    "    .agg(\n",
    "        F.avg(\"air_temperature\").alias(\"s_air\"),\n",
    "        F.avg(\"cloud_coverage\").alias(\"s_cloud\"),\n",
    "        F.avg(\"dew_temperature\").alias(\"s_dew\"),\n",
    "        F.avg(\"sea_level_pressure\").alias(\"s_slp\"),\n",
    "        F.avg(\"wind_speed\").alias(\"s_wspd\"),\n",
    "        F.count(F.lit(1)).alias(\"n_obs\"),\n",
    "        F.atan2(\n",
    "            F.avg(F.sin(F.radians(\"wind_direction\"))),\n",
    "            F.avg(F.cos(F.radians(\"wind_direction\")))\n",
    "        ).alias(\"s_wdir_rad\")\n",
    "    )\n",
    "    .withColumn(\"s_wdir\", ((F.degrees(\"s_wdir_rad\") + F.lit(360.0)) % F.lit(360.0)))\n",
    "    .drop(\"s_wdir_rad\")\n",
    "    .withColumn(\n",
    "        \"peak_flag\",\n",
    "        F.when(F.col(\"slot\").isin(1,2), F.lit(\"peak\")).otherwise(F.lit(\"off-peak\"))\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tPerform the necessary transformation you used in A2A. (note: every student may have used different features, feel free to reuse the code you have written in A2A. If you built an end-to-end pipeline, you can ignore this task.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add hour-of-day and 6-hour interval slot\n",
    "meters_with_slot = (\n",
    "    meters_df\n",
    "      .withColumn(\"hour\", F.hour(\"ts\"))\n",
    "      .withColumn(\"slot\", (F.col(\"hour\") / 6).cast(\"int\"))  # 0,1,2,3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date for grouping\n",
    "meters_with_slot = meters_with_slot.withColumn(\"date\", F.to_date(\"ts\"))\n",
    "\n",
    "# Aggregate\n",
    "agg_df = (\n",
    "    meters_with_slot\n",
    "      .groupBy(\"building_id\", \"date\", \"slot\")\n",
    "      .agg(F.sum(\"value\").alias(\"energy_6h\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = agg_df.withColumn(\n",
    "    \"slot_label\",\n",
    "    F.when(F.col(\"slot\") == 0, \"00:00-05:59\")\n",
    "     .when(F.col(\"slot\") == 1, \"06:00-11:59\")\n",
    "     .when(F.col(\"slot\") == 2, \"12:00-17:59\")\n",
    "     .when(F.col(\"slot\") == 3, \"18:00-23:59\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-hour aggregated energy consumption:\n",
      "+-----------+----------+----+------------+-----------+\n",
      "|building_id|date      |slot|energy_6h   |slot_label |\n",
      "+-----------+----------+----+------------+-----------+\n",
      "|194        |2022-01-01|0   |1746.996100 |00:00-05:59|\n",
      "|260        |2022-01-01|0   |2472.028500 |00:00-05:59|\n",
      "|889        |2022-01-01|0   |1960.266000 |00:00-05:59|\n",
      "|895        |2022-01-01|0   |998.004900  |00:00-05:59|\n",
      "|926        |2022-01-01|0   |247.469100  |00:00-05:59|\n",
      "|931        |2022-01-01|0   |2898.023600 |00:00-05:59|\n",
      "|933        |2022-01-01|0   |205.366700  |00:00-05:59|\n",
      "|952        |2022-01-01|0   |6129.160000 |00:00-05:59|\n",
      "|973        |2022-01-01|0   |1973.255500 |00:00-05:59|\n",
      "|1092       |2022-01-01|0   |10067.018600|00:00-05:59|\n",
      "|1140       |2022-01-01|0   |43435.750400|00:00-05:59|\n",
      "|1160       |2022-01-01|0   |132.299000  |00:00-05:59|\n",
      "+-----------+----------+----+------------+-----------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"6-hour aggregated energy consumption:\")\n",
    "agg_df.show(12, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tLoad your pipeline model and perform the following aggregations:  \n",
    "a)\tPrint the prediction from your model as a stream comes in.  \n",
    "b)\tEvery 7 seconds, print the total energy consumption for each 6-hour interval, aggregated by building, and print 20 records. (Note: This is simulating energy data each day in a week)  \n",
    "c)\tEvery 14 seconds, for each site, print the daily total energy consumption.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 0: Spark tuning & safety =====\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Kill any old queries to avoid resource contention\n",
    "for q in list(spark.streams.active):\n",
    "    print(\"stopping\", q.name); q.stop()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# Practical defaults\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")   # tune 8–16 based on cores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CELL 1: Config, model, helpers =====\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "from pyspark.ml import PipelineModel\n",
    "import uuid, datetime as dt, os, math, time\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "MODEL_PATH = \"models/gbt_best_model\"           # change if needed\n",
    "BASE = \"/tmp/stream_jobs\"                      # use a host mount if you want persistence\n",
    "CKPT_ONE   = f\"{BASE}/ckpt_one_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "# Optional batch Parquet outputs (inside foreachBatch; set to False to disable)\n",
    "WRITE_PARQUET_B = False   # 6h-by-building sink\n",
    "WRITE_PARQUET_C = False   # daily-by-site sink\n",
    "OUT_B = f\"{BASE}/out/energy_6h_by_building\"\n",
    "OUT_C = f\"{BASE}/out/daily_energy_by_site\"\n",
    "\n",
    "# Predictions clamp (avoid negative & extreme outliers)\n",
    "CLAMP_CAP = 1e6\n",
    "PRINT_SAMPLE_ROWS = 10\n",
    "\n",
    "# ---------- ASSUMED INPUTS (already exist in your session) ----------\n",
    "# - weather_6h: streaming DF with [site_id,date,slot,slot_label,s_air,s_dew,s_wspd,s_cloud,s_slp,...]\n",
    "# - buildings_df: static DF with [building_id,site_id,square_feet,floor_count,primary_use, (optional) latent_*]\n",
    "# - agg_df: static 6h energy DF with [building_id,date,slot,energy_6h,slot_label]\n",
    "\n",
    "# ---------- UTIL: tiny inspectors ----------\n",
    "def nonempty(df): \n",
    "    try: \n",
    "        return not df.rdd.isEmpty()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _print_stats(tag, batch_id, df):\n",
    "    try:\n",
    "        n = df.count()\n",
    "    except Exception:\n",
    "        n = 0\n",
    "    print(f\"\\n[{tag}] batch {batch_id} | rows={n}\", flush=True)\n",
    "\n",
    "# ---------- Load model & figure out assembler inputs ----------\n",
    "model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "assembler_input_cols = []\n",
    "model_output_cols = {\"prediction\",\"rawPrediction\",\"probability\"}\n",
    "for stg in model.stages:\n",
    "    cname = stg.__class__.__name__\n",
    "    if cname == \"VectorAssembler\":\n",
    "        assembler_input_cols = list(stg.getInputCols())\n",
    "        model_output_cols.add(stg.getOutputCol())\n",
    "    elif \"StringIndexerModel\" in cname:\n",
    "        try:\n",
    "            model_output_cols.add(stg.getOutputCol())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def ensure_model_inputs(df):\n",
    "    for c in assembler_input_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(0.0).cast(\"double\"))\n",
    "    return df\n",
    "\n",
    "def drop_model_outputs_if_exist(df):\n",
    "    return df.drop(*[c for c in model_output_cols if c in df.columns])\n",
    "\n",
    "# ---------- Precompute lags/rolling from full historic energy (STATIC, once) ----------\n",
    "w = W.partitionBy(\"building_id\").orderBy(\"date\",\"slot\")\n",
    "energy_lagged = (\n",
    "    agg_df\n",
    "    .withColumn(\"lag_1\",       F.lag(\"energy_6h\", 1).over(w))\n",
    "    .withColumn(\"lag_4\",       F.lag(\"energy_6h\", 4).over(w))\n",
    "    .withColumn(\"lag_28\",      F.lag(\"energy_6h\", 28).over(w))\n",
    "    .withColumn(\"roll_mean_4\", F.avg(\"energy_6h\").over(w.rowsBetween(-4, -1)))\n",
    "    .withColumn(\"roll_std_4\",  F.stddev(\"energy_6h\").over(w.rowsBetween(-4, -1)))\n",
    "    .fillna({\"lag_1\":0.0,\"lag_4\":0.0,\"lag_28\":0.0,\"roll_mean_4\":0.0,\"roll_std_4\":0.0})\n",
    ")\n",
    "\n",
    "energy_enriched = (\n",
    "    energy_lagged\n",
    "    .join(buildings_df.select(\"building_id\",\"site_id\"), on=\"building_id\", how=\"left\")\n",
    "    .select(\"building_id\",\"site_id\",\"date\",\"slot\",\"energy_6h\",\n",
    "            \"lag_1\",\"lag_4\",\"lag_28\",\"roll_mean_4\",\"roll_std_4\")\n",
    ")\n",
    "\n",
    "# ---------- Join weather × buildings (per micro-batch) ----------\n",
    "buildings_slim = buildings_df.select(\n",
    "    \"building_id\",\"site_id\",\"square_feet\",\"floor_count\",\"primary_use\",\n",
    "    *[c for c in [\"latent_y\",\"latent_s\",\"latent_r\"] if c in buildings_df.columns]\n",
    ")\n",
    "\n",
    "def join_weather_buildings(batch_weather):\n",
    "    return (\n",
    "        batch_weather\n",
    "        .select(\n",
    "            \"site_id\",\"date\",\"slot\",\"slot_label\",\n",
    "            F.col(\"s_air\").alias(\"avg_air_temp\"),\n",
    "            F.col(\"s_dew\").alias(\"avg_dew_temp\"),\n",
    "            F.col(\"s_wspd\").alias(\"avg_wind_speed\"),\n",
    "            F.col(\"s_cloud\").alias(\"avg_cloud_coverage\"),\n",
    "            F.col(\"s_slp\").alias(\"avg_sea_level_pressure\"),\n",
    "        )\n",
    "        .withColumn(\"site_id\", F.col(\"site_id\").cast(\"int\"))\n",
    "        .withColumn(\"slot\",    F.col(\"slot\").cast(\"int\"))\n",
    "        .join(F.broadcast(buildings_slim.withColumn(\"site_id\", F.col(\"site_id\").cast(\"int\"))),\n",
    "              on=\"site_id\", how=\"left\")\n",
    "        .withColumn(\"primary_use\", F.coalesce(F.col(\"primary_use\"), F.lit(\"unknown\")))\n",
    "    )\n",
    "\n",
    "# ---------- Feature shaping ----------\n",
    "def shape_for_model_from_joined(joined):\n",
    "    base = (\n",
    "        joined\n",
    "        .withColumn(\"peak_flag\",\n",
    "            F.when(F.col(\"slot_label\").isin(\"06:00-11:59\",\"12:00-17:59\"), \"peak\").otherwise(\"off-peak\"))\n",
    "        .withColumn(\"slot_sin\", F.sin(2*math.pi*F.col(\"slot\")/F.lit(4)))\n",
    "        .withColumn(\"slot_cos\", F.cos(2*math.pi*F.col(\"slot\")/F.lit(4)))\n",
    "        .withColumn(\"dow\", F.dayofweek(\"date\"))\n",
    "        .withColumn(\"dow_sin\", F.sin(2*math.pi*(F.col(\"dow\")-1)/F.lit(7)))\n",
    "        .withColumn(\"dow_cos\", F.cos(2*math.pi*(F.col(\"dow\")-1)/F.lit(7)))\n",
    "        .withColumn(\"cdh\", F.greatest(F.col(\"avg_air_temp\")-F.lit(18.0), F.lit(0.0)))\n",
    "        .withColumn(\"hdh\", F.greatest(F.lit(18.0)-F.col(\"avg_air_temp\"), F.lit(0.0)))\n",
    "        .withColumn(\"sum_weather_core\",\n",
    "            F.coalesce(F.col(\"avg_air_temp\"),F.lit(0.0)) +\n",
    "            F.coalesce(F.col(\"avg_dew_temp\"),F.lit(0.0)) +\n",
    "            F.coalesce(F.col(\"avg_wind_speed\"),F.lit(0.0)))\n",
    "    )\n",
    "\n",
    "    # ensure lag/rolling columns exist\n",
    "    for c in [\"lag_1\",\"lag_4\",\"lag_28\",\"roll_mean_4\",\"roll_std_4\"]:\n",
    "        if c not in base.columns:\n",
    "            base = base.withColumn(c, F.lit(0.0))\n",
    "\n",
    "    # weather variance features\n",
    "    nW = F.lit(5.0)\n",
    "    sq_sum = (\n",
    "        F.pow(F.coalesce(F.col(\"avg_air_temp\"),F.lit(0.0)),2) +\n",
    "        F.pow(F.coalesce(F.col(\"avg_dew_temp\"),F.lit(0.0)),2) +\n",
    "        F.pow(F.coalesce(F.col(\"avg_wind_speed\"),F.lit(0.0)),2) +\n",
    "        F.pow(F.coalesce(F.col(\"avg_cloud_coverage\"),F.lit(0.0)),2) +\n",
    "        F.pow(F.coalesce(F.col(\"avg_sea_level_pressure\"),F.lit(0.0)),2)\n",
    "    )\n",
    "    mean = (\n",
    "        F.coalesce(F.col(\"avg_air_temp\"),F.lit(0.0)) +\n",
    "        F.coalesce(F.col(\"avg_dew_temp\"),F.lit(0.0)) +\n",
    "        F.coalesce(F.col(\"avg_wind_speed\"),F.lit(0.0)) +\n",
    "        F.coalesce(F.col(\"avg_cloud_coverage\"),F.lit(0.0)) +\n",
    "        F.coalesce(F.col(\"avg_sea_level_pressure\"),F.lit(0.0))\n",
    "    )/nW\n",
    "    var = (sq_sum/nW) - (mean*mean)\n",
    "\n",
    "    base = (base\n",
    "        .withColumn(\"std_weather\", F.sqrt(F.when(var < 0, 0.0).otherwise(var)))\n",
    "        .withColumn(\"root_ratio_weather_var_over_size\",\n",
    "                    F.sqrt((F.col(\"std_weather\")**2) /\n",
    "                           (F.coalesce(F.col(\"square_feet\"),F.lit(0.0)) + F.lit(1e-6))))\n",
    "        .withColumn(\"root_ratio_rollstd_over_size\",\n",
    "                    F.sqrt(F.coalesce(F.col(\"roll_std_4\"),F.lit(0.0)) /\n",
    "                           (F.coalesce(F.col(\"square_feet\"),F.lit(0.0)) + F.lit(1e-6))))\n",
    "    )\n",
    "\n",
    "    for lc in (\"latent_y\",\"latent_s\",\"latent_r\"):\n",
    "        if lc not in base.columns:\n",
    "            base = base.withColumn(lc, F.lit(0.0).cast(\"double\"))\n",
    "\n",
    "    base = base.fillna({\n",
    "        \"square_feet\": 0.0, \"floor_count\": 0.0,\n",
    "        \"avg_air_temp\": 0.0, \"avg_dew_temp\": 0.0, \"avg_wind_speed\": 0.0,\n",
    "        \"avg_cloud_coverage\": 0.0, \"avg_sea_level_pressure\": 0.0,\n",
    "        \"lag_1\": 0.0, \"lag_4\": 0.0, \"lag_28\": 0.0, \"roll_mean_4\": 0.0, \"roll_std_4\": 0.0\n",
    "    })\n",
    "\n",
    "    base = ensure_model_inputs(base)\n",
    "    base = drop_model_outputs_if_exist(base)\n",
    "    return base.withColumn(\"label\", F.lit(None).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch 0] no new rows\n",
      "\n",
      "--- (a) predictions — batch 1 ---\n",
      "{'id': '5f589a5d-6ce6-4c29-8a96-d0cc054d9d6a', 'runId': '006c2f43-69ce-403d-992a-59b1ec7a7c8c', 'name': 'one_stream_three_tasks', 'timestamp': '2025-10-25T22:52:08.404Z', 'batchId': 0, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'durationMs': {'addBatch': 429, 'commitOffsets': 16, 'getBatch': 0, 'latestOffset': 60, 'queryPlanning': 18, 'triggerExecution': 565, 'walCommit': 38}, 'eventTime': {'watermark': '1970-01-01T00:00:00.000Z'}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 0, 'numRowsUpdated': 0, 'allUpdatesTimeMs': 52, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 207, 'memoryUsedBytes': 1792, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 8, 'numStateStoreInstances': 8, 'customMetrics': {'loadedMapCacheHitCount': 0, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 640}}, {'operatorName': 'dedupe', 'numRowsTotal': 0, 'numRowsUpdated': 0, 'allUpdatesTimeMs': 30, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 336, 'memoryUsedBytes': 1792, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 8, 'numStateStoreInstances': 8, 'customMetrics': {'loadedMapCacheHitCount': 0, 'loadedMapCacheMissCount': 0, 'numDroppedDuplicateRows': 0, 'stateOnCurrentVersionSizeBytes': 640}}], 'sources': [{'description': 'KafkaV2[SubscribePattern[weather-site-\\\\d+]]', 'startOffset': None, 'endOffset': {'weather-site-0': {'0': 338}, 'weather-site-3': {'0': 292}, 'weather-site-6': {'0': 90}, 'weather-site-5': {'0': 260}, 'weather-site-2': {'0': 292}, 'weather-site-1': {'0': 292}, 'weather-site-4': {'0': 292}, 'weather-site-7': {'0': 73}}, 'latestOffset': {'weather-site-0': {'0': 338}, 'weather-site-3': {'0': 292}, 'weather-site-6': {'0': 90}, 'weather-site-5': {'0': 260}, 'weather-site-2': {'0': 292}, 'weather-site-1': {'0': 292}, 'weather-site-4': {'0': 292}, 'weather-site-7': {'0': 73}}, 'numInputRows': 0, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 0.0, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-08-19|0   |11687.217094702759|\n",
      "|7          |0      |2022-08-19|0   |4574.489311491051 |\n",
      "|11         |0      |2022-08-19|0   |2951.662716071554 |\n",
      "|14         |0      |2022-08-19|0   |6070.670218895307 |\n",
      "|15         |0      |2022-08-19|0   |11073.898897298503|\n",
      "|19         |0      |2022-08-19|0   |1331.8485989187102|\n",
      "|25         |0      |2022-08-19|0   |2951.662716071554 |\n",
      "|35         |0      |2022-08-19|0   |1331.8485989187102|\n",
      "|42         |0      |2022-08-19|0   |8157.238547299251 |\n",
      "|44         |0      |2022-08-19|0   |90.69999381569704 |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 1 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-08-19|0   |11687.217094702759|\n",
      "|7          |2022-08-19|0   |4574.489311491051 |\n",
      "|11         |2022-08-19|0   |2951.662716071554 |\n",
      "|14         |2022-08-19|0   |6070.670218895307 |\n",
      "|15         |2022-08-19|0   |11073.898897298503|\n",
      "|19         |2022-08-19|0   |1331.8485989187102|\n",
      "|25         |2022-08-19|0   |2951.662716071554 |\n",
      "|35         |2022-08-19|0   |1331.8485989187102|\n",
      "|42         |2022-08-19|0   |8157.238547299251 |\n",
      "|44         |2022-08-19|0   |90.69999381569704 |\n",
      "|53         |2022-08-19|0   |90.69999381569704 |\n",
      "|57         |2022-08-19|0   |1331.8485989187102|\n",
      "|60         |2022-08-19|0   |4574.489311491051 |\n",
      "|61         |2022-08-19|0   |1331.8485989187102|\n",
      "|62         |2022-08-19|0   |2951.662716071554 |\n",
      "|64         |2022-08-19|0   |248.07727084846331|\n",
      "|68         |2022-08-19|0   |539.1816359690715 |\n",
      "|75         |2022-08-19|0   |1331.8485989187102|\n",
      "|76         |2022-08-19|0   |35077.19896200204 |\n",
      "|77         |2022-08-19|0   |248.07727084846331|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "--- (a) predictions — batch 2 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-08-24|0   |11687.217094702759|\n",
      "|7          |0      |2022-08-24|0   |4574.489311491051 |\n",
      "|11         |0      |2022-08-24|0   |2951.662716071554 |\n",
      "|14         |0      |2022-08-24|0   |4574.489311491051 |\n",
      "|15         |0      |2022-08-24|0   |12374.51618811274 |\n",
      "|19         |0      |2022-08-24|0   |1331.8485989187102|\n",
      "|25         |0      |2022-08-24|0   |2951.662716071554 |\n",
      "|35         |0      |2022-08-24|0   |1331.8485989187102|\n",
      "|42         |0      |2022-08-24|0   |8157.238547299251 |\n",
      "|44         |0      |2022-08-24|0   |0.0               |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 2 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-08-24|0   |11687.217094702759|\n",
      "|7          |2022-08-24|0   |4574.489311491051 |\n",
      "|11         |2022-08-24|0   |2951.662716071554 |\n",
      "|14         |2022-08-24|0   |4574.489311491051 |\n",
      "|15         |2022-08-24|0   |12374.51618811274 |\n",
      "|19         |2022-08-24|0   |1331.8485989187102|\n",
      "|25         |2022-08-24|0   |2951.662716071554 |\n",
      "|35         |2022-08-24|0   |1331.8485989187102|\n",
      "|42         |2022-08-24|0   |8157.238547299251 |\n",
      "|44         |2022-08-24|0   |0.0               |\n",
      "|53         |2022-08-24|0   |90.69999381569704 |\n",
      "|57         |2022-08-24|0   |2951.662716071554 |\n",
      "|60         |2022-08-24|0   |4574.489311491051 |\n",
      "|61         |2022-08-24|0   |1331.8485989187102|\n",
      "|62         |2022-08-24|0   |2951.662716071554 |\n",
      "|64         |2022-08-24|0   |1331.8485989187102|\n",
      "|68         |2022-08-24|0   |1622.9529640393184|\n",
      "|75         |2022-08-24|0   |1331.8485989187102|\n",
      "|76         |2022-08-24|0   |35077.19896200204 |\n",
      "|77         |2022-08-24|0   |248.07727084846331|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "--- (c) daily energy by site — batch 2 ---\n",
      "+-------+----------+------------------+\n",
      "|site_id|date      |daily_energy      |\n",
      "+-------+----------+------------------+\n",
      "|0      |2022-08-24|1038117.3023945255|\n",
      "|0      |2022-08-25|1071612.0546553219|\n",
      "|0      |2022-08-26|1108532.2368404125|\n",
      "|0      |2022-08-27|1105030.1382004418|\n",
      "|0      |2022-08-28|1124066.185547542 |\n",
      "|0      |2022-08-29|1230088.661666796 |\n",
      "|0      |2022-08-30|1212714.122229215 |\n",
      "|0      |2022-08-31|1058204.2262998826|\n",
      "|0      |2022-09-01|937258.6813978734 |\n",
      "|0      |2022-09-02|1057188.3046286872|\n",
      "+-------+----------+------------------+\n",
      "\n",
      "\n",
      "--- (a) predictions — batch 3 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-09-03|0   |11687.217094702759|\n",
      "|7          |0      |2022-09-03|0   |4574.489311491051 |\n",
      "|11         |0      |2022-09-03|0   |2951.662716071554 |\n",
      "|14         |0      |2022-09-03|0   |12374.51618811274 |\n",
      "|15         |0      |2022-09-03|0   |11687.217094702759|\n",
      "|19         |0      |2022-09-03|0   |1331.8485989187102|\n",
      "|25         |0      |2022-09-03|0   |2951.662716071554 |\n",
      "|35         |0      |2022-09-03|0   |1331.8485989187102|\n",
      "|42         |0      |2022-09-03|0   |8157.238547299251 |\n",
      "|44         |0      |2022-09-03|0   |0.0               |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 3 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-09-03|0   |11687.217094702759|\n",
      "|7          |2022-09-03|0   |4574.489311491051 |\n",
      "|11         |2022-09-03|0   |2951.662716071554 |\n",
      "|14         |2022-09-03|0   |12374.51618811274 |\n",
      "|15         |2022-09-03|0   |11687.217094702759|\n",
      "|19         |2022-09-03|0   |1331.8485989187102|\n",
      "|25         |2022-09-03|0   |2951.662716071554 |\n",
      "|35         |2022-09-03|0   |1331.8485989187102|\n",
      "|42         |2022-09-03|0   |8157.238547299251 |\n",
      "|44         |2022-09-03|0   |0.0               |\n",
      "|53         |2022-09-03|0   |90.69999381569704 |\n",
      "|57         |2022-09-03|0   |2951.662716071554 |\n",
      "|60         |2022-09-03|0   |4574.489311491051 |\n",
      "|61         |2022-09-03|0   |1331.8485989187102|\n",
      "|62         |2022-09-03|0   |2951.662716071554 |\n",
      "|64         |2022-09-03|0   |1331.8485989187102|\n",
      "|68         |2022-09-03|0   |1622.9529640393184|\n",
      "|75         |2022-09-03|0   |6070.670218895307 |\n",
      "|76         |2022-09-03|0   |35077.19896200204 |\n",
      "|77         |2022-09-03|0   |248.07727084846331|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- (a) predictions — batch 4 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-09-08|0   |11687.217094702759|\n",
      "|7          |0      |2022-09-08|0   |4574.489311491051 |\n",
      "|11         |0      |2022-09-08|0   |2951.662716071554 |\n",
      "|14         |0      |2022-09-08|0   |6070.670218895307 |\n",
      "|15         |0      |2022-09-08|0   |12374.51618811274 |\n",
      "|19         |0      |2022-09-08|0   |1331.8485989187102|\n",
      "|25         |0      |2022-09-08|0   |2951.662716071554 |\n",
      "|35         |0      |2022-09-08|0   |1331.8485989187102|\n",
      "|42         |0      |2022-09-08|0   |8157.238547299251 |\n",
      "|44         |0      |2022-09-08|0   |0.0               |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 4 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-09-08|0   |11687.217094702759|\n",
      "|7          |2022-09-08|0   |4574.489311491051 |\n",
      "|11         |2022-09-08|0   |2951.662716071554 |\n",
      "|14         |2022-09-08|0   |6070.670218895307 |\n",
      "|15         |2022-09-08|0   |12374.51618811274 |\n",
      "|19         |2022-09-08|0   |1331.8485989187102|\n",
      "|25         |2022-09-08|0   |2951.662716071554 |\n",
      "|35         |2022-09-08|0   |1331.8485989187102|\n",
      "|42         |2022-09-08|0   |8157.238547299251 |\n",
      "|44         |2022-09-08|0   |0.0               |\n",
      "|53         |2022-09-08|0   |90.69999381569704 |\n",
      "|57         |2022-09-08|0   |2951.662716071554 |\n",
      "|60         |2022-09-08|0   |4574.489311491051 |\n",
      "|61         |2022-09-08|0   |1331.8485989187102|\n",
      "|62         |2022-09-08|0   |2951.662716071554 |\n",
      "|64         |2022-09-08|0   |1331.8485989187102|\n",
      "|68         |2022-09-08|0   |1622.9529640393184|\n",
      "|75         |2022-09-08|0   |6070.670218895307 |\n",
      "|76         |2022-09-08|0   |36213.41809701366 |\n",
      "|77         |2022-09-08|0   |248.07727084846331|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "--- (c) daily energy by site — batch 4 ---\n",
      "+-------+----------+------------------+\n",
      "|site_id|date      |daily_energy      |\n",
      "+-------+----------+------------------+\n",
      "|0      |2022-09-08|1218294.2777331553|\n",
      "|0      |2022-09-09|1264995.8030263493|\n",
      "|0      |2022-09-10|1269696.1805160039|\n",
      "|0      |2022-09-11|1176643.0154740498|\n",
      "|0      |2022-09-12|1210415.9207060696|\n",
      "+-------+----------+------------------+\n",
      "\n",
      "\n",
      "--- (a) predictions — batch 5 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-09-13|0   |11687.217094702759|\n",
      "|7          |0      |2022-09-13|0   |0.0               |\n",
      "|11         |0      |2022-09-13|0   |2951.662716071554 |\n",
      "|14         |0      |2022-09-13|0   |0.0               |\n",
      "|15         |0      |2022-09-13|0   |0.0               |\n",
      "|19         |0      |2022-09-13|0   |1331.8485989187102|\n",
      "|25         |0      |2022-09-13|0   |2951.662716071554 |\n",
      "|35         |0      |2022-09-13|0   |1331.8485989187102|\n",
      "|42         |0      |2022-09-13|0   |8157.238547299251 |\n",
      "|44         |0      |2022-09-13|0   |0.0               |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 5 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-09-13|0   |11687.217094702759|\n",
      "|7          |2022-09-13|0   |0.0               |\n",
      "|11         |2022-09-13|0   |2951.662716071554 |\n",
      "|14         |2022-09-13|0   |0.0               |\n",
      "|15         |2022-09-13|0   |0.0               |\n",
      "|19         |2022-09-13|0   |1331.8485989187102|\n",
      "|25         |2022-09-13|0   |2951.662716071554 |\n",
      "|35         |2022-09-13|0   |1331.8485989187102|\n",
      "|42         |2022-09-13|0   |8157.238547299251 |\n",
      "|44         |2022-09-13|0   |0.0               |\n",
      "|53         |2022-09-13|0   |90.69999381569704 |\n",
      "|57         |2022-09-13|0   |2951.662716071554 |\n",
      "|60         |2022-09-13|0   |0.0               |\n",
      "|61         |2022-09-13|0   |1331.8485989187102|\n",
      "|62         |2022-09-13|0   |2951.662716071554 |\n",
      "|64         |2022-09-13|0   |1331.8485989187102|\n",
      "|68         |2022-09-13|0   |1622.9529640393184|\n",
      "|75         |2022-09-13|0   |0.0               |\n",
      "|76         |2022-09-13|0   |38044.56375876955 |\n",
      "|77         |2022-09-13|0   |248.07727084846331|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "--- (a) predictions — batch 6 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-09-23|0   |11687.217094702759|\n",
      "|7          |0      |2022-09-23|0   |4574.489311491051 |\n",
      "|11         |0      |2022-09-23|0   |2951.662716071554 |\n",
      "|14         |0      |2022-09-23|0   |0.0               |\n",
      "|15         |0      |2022-09-23|0   |12374.51618811274 |\n",
      "|19         |0      |2022-09-23|0   |1331.8485989187102|\n",
      "|25         |0      |2022-09-23|0   |2951.662716071554 |\n",
      "|35         |0      |2022-09-23|0   |1331.8485989187102|\n",
      "|42         |0      |2022-09-23|0   |8157.238547299251 |\n",
      "|44         |0      |2022-09-23|0   |0.0               |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 6 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-09-23|0   |11687.217094702759|\n",
      "|7          |2022-09-23|0   |4574.489311491051 |\n",
      "|11         |2022-09-23|0   |2951.662716071554 |\n",
      "|14         |2022-09-23|0   |0.0               |\n",
      "|15         |2022-09-23|0   |12374.51618811274 |\n",
      "|19         |2022-09-23|0   |1331.8485989187102|\n",
      "|25         |2022-09-23|0   |2951.662716071554 |\n",
      "|35         |2022-09-23|0   |1331.8485989187102|\n",
      "|42         |2022-09-23|0   |8157.238547299251 |\n",
      "|44         |2022-09-23|0   |0.0               |\n",
      "|53         |2022-09-23|0   |90.69999381569704 |\n",
      "|57         |2022-09-23|0   |2951.662716071554 |\n",
      "|60         |2022-09-23|0   |4574.489311491051 |\n",
      "|61         |2022-09-23|0   |1331.8485989187102|\n",
      "|62         |2022-09-23|0   |2951.662716071554 |\n",
      "|64         |2022-09-23|0   |1331.8485989187102|\n",
      "|68         |2022-09-23|0   |1622.9529640393184|\n",
      "|75         |2022-09-23|0   |6070.670218895307 |\n",
      "|76         |2022-09-23|0   |36213.41809701366 |\n",
      "|77         |2022-09-23|0   |248.07727084846331|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "--- (c) daily energy by site — batch 6 ---\n",
      "+-------+----------+------------------+\n",
      "|site_id|date      |daily_energy      |\n",
      "+-------+----------+------------------+\n",
      "|0      |2022-09-23|1118051.1303046467|\n",
      "|0      |2022-09-24|1078805.9015291452|\n",
      "|0      |2022-09-25|916523.3176952123 |\n",
      "|0      |2022-09-26|1032687.6822145056|\n",
      "|0      |2022-09-27|1150802.611699991 |\n",
      "+-------+----------+------------------+\n",
      "\n",
      "\n",
      "--- (a) predictions — batch 7 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-09-28|0   |11687.217094702759|\n",
      "|7          |0      |2022-09-28|0   |4574.489311491051 |\n",
      "|11         |0      |2022-09-28|0   |2951.662716071554 |\n",
      "|14         |0      |2022-09-28|0   |0.0               |\n",
      "|15         |0      |2022-09-28|0   |12374.51618811274 |\n",
      "|19         |0      |2022-09-28|0   |1331.8485989187102|\n",
      "|25         |0      |2022-09-28|0   |2951.662716071554 |\n",
      "|35         |0      |2022-09-28|0   |1331.8485989187102|\n",
      "|42         |0      |2022-09-28|0   |8157.238547299251 |\n",
      "|44         |0      |2022-09-28|0   |0.0               |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 7 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-09-28|0   |11687.217094702759|\n",
      "|7          |2022-09-28|0   |4574.489311491051 |\n",
      "|11         |2022-09-28|0   |2951.662716071554 |\n",
      "|14         |2022-09-28|0   |0.0               |\n",
      "|15         |2022-09-28|0   |12374.51618811274 |\n",
      "|19         |2022-09-28|0   |1331.8485989187102|\n",
      "|25         |2022-09-28|0   |2951.662716071554 |\n",
      "|35         |2022-09-28|0   |1331.8485989187102|\n",
      "|42         |2022-09-28|0   |8157.238547299251 |\n",
      "|44         |2022-09-28|0   |0.0               |\n",
      "|53         |2022-09-28|0   |90.69999381569704 |\n",
      "|57         |2022-09-28|0   |2951.662716071554 |\n",
      "|60         |2022-09-28|0   |4574.489311491051 |\n",
      "|61         |2022-09-28|0   |1331.8485989187102|\n",
      "|62         |2022-09-28|0   |2951.662716071554 |\n",
      "|64         |2022-09-28|0   |1331.8485989187102|\n",
      "|68         |2022-09-28|0   |1622.9529640393184|\n",
      "|75         |2022-09-28|0   |6070.670218895307 |\n",
      "|76         |2022-09-28|0   |36213.41809701366 |\n",
      "|77         |2022-09-28|0   |1331.8485989187102|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- (a) predictions — batch 8 ---\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|building_id|site_id|date      |slot|energy            |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "|4          |0      |2022-10-08|0   |8157.238547299251 |\n",
      "|7          |0      |2022-10-08|0   |0.0               |\n",
      "|11         |0      |2022-10-08|0   |2951.662716071554 |\n",
      "|14         |0      |2022-10-08|0   |84253.46804401917 |\n",
      "|15         |0      |2022-10-08|0   |90557.3140132366  |\n",
      "|19         |0      |2022-10-08|0   |1331.8485989187102|\n",
      "|25         |0      |2022-10-08|0   |1331.8485989187102|\n",
      "|35         |0      |2022-10-08|0   |1331.8485989187102|\n",
      "|42         |0      |2022-10-08|0   |5383.371125485325 |\n",
      "|44         |0      |2022-10-08|0   |90.69999381569704 |\n",
      "+-----------+-------+----------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "--- (b) 6h energy by building — batch 8 ---\n",
      "+-----------+----------+----+------------------+\n",
      "|building_id|date      |slot|energy_6h_total   |\n",
      "+-----------+----------+----+------------------+\n",
      "|4          |2022-10-08|0   |8157.238547299251 |\n",
      "|7          |2022-10-08|0   |0.0               |\n",
      "|11         |2022-10-08|0   |2951.662716071554 |\n",
      "|14         |2022-10-08|0   |84253.46804401917 |\n",
      "|15         |2022-10-08|0   |90557.3140132366  |\n",
      "|19         |2022-10-08|0   |1331.8485989187102|\n",
      "|25         |2022-10-08|0   |1331.8485989187102|\n",
      "|35         |2022-10-08|0   |1331.8485989187102|\n",
      "|42         |2022-10-08|0   |5383.371125485325 |\n",
      "|44         |2022-10-08|0   |90.69999381569704 |\n",
      "|53         |2022-10-08|0   |90.69999381569704 |\n",
      "|57         |2022-10-08|0   |1331.8485989187102|\n",
      "|60         |2022-10-08|0   |97809.8721022704  |\n",
      "|61         |2022-10-08|0   |1331.8485989187102|\n",
      "|62         |2022-10-08|0   |2951.662716071554 |\n",
      "|64         |2022-10-08|0   |248.07727084846331|\n",
      "|68         |2022-10-08|0   |539.1816359690715 |\n",
      "|75         |2022-10-08|0   |84253.46804401917 |\n",
      "|76         |2022-10-08|0   |0.0               |\n",
      "|77         |2022-10-08|0   |1331.8485989187102|\n",
      "+-----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "--- (c) daily energy by site — batch 8 ---\n",
      "+-------+----------+------------------+\n",
      "|site_id|date      |daily_energy      |\n",
      "+-------+----------+------------------+\n",
      "|0      |2022-10-08|2349237.306182955 |\n",
      "|0      |2022-10-09|1035129.0469330584|\n",
      "|0      |2022-10-10|907131.6645027364 |\n",
      "|0      |2022-10-11|795522.9671314254 |\n",
      "|0      |2022-10-12|892958.5599167228 |\n",
      "+-------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 2: One stream, three tasks (7s + 14s) =====\n",
    "import os, time\n",
    "\n",
    "def process_all(batch_df, epochId: int):\n",
    "    # Quick empty check\n",
    "    if batch_df.head(1) == []:\n",
    "        print(f\"\\n[batch {epochId}] no new rows\"); \n",
    "        return\n",
    "\n",
    "    # --- common prep (once per micro-batch) ---\n",
    "    wb = join_weather_buildings(batch_df)\n",
    "    wb_lag = (\n",
    "        wb.join(\n",
    "            F.broadcast(\n",
    "                energy_enriched.select(\n",
    "                    \"building_id\",\"date\",\"slot\",\n",
    "                    \"lag_1\",\"lag_4\",\"lag_28\",\"roll_mean_4\",\"roll_std_4\"\n",
    "                )\n",
    "            ),\n",
    "            on=[\"building_id\",\"date\",\"slot\"], how=\"left\"\n",
    "        )\n",
    "        .fillna({\"lag_1\":0.0,\"lag_4\":0.0,\"lag_28\":0.0,\"roll_mean_4\":0.0,\"roll_std_4\":0.0})\n",
    "    )\n",
    "    shaped = drop_model_outputs_if_exist(shape_for_model_from_joined(wb_lag))\n",
    "\n",
    "    # --- (a) Predictions (every batch) ---\n",
    "    preds = (\n",
    "        model.transform(shaped)\n",
    "        .select(\"building_id\",\"site_id\",\"date\",\"slot\",\n",
    "                F.col(\"prediction\").alias(\"energy_raw\"))\n",
    "        .withColumn(\n",
    "            \"energy\",\n",
    "            F.greatest(F.lit(0.0), F.least(F.col(\"energy_raw\"), F.lit(CLAMP_CAP)))\n",
    "        )\n",
    "        .drop(\"energy_raw\")\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- (a) predictions — batch {epochId} ---\")\n",
    "    preds.orderBy(\"date\",\"slot\",\"building_id\").show(PRINT_SAMPLE_ROWS, truncate=False)\n",
    "\n",
    "    # --- (b) 6h energy by building (every batch ~7s) ---\n",
    "    six = (preds.groupBy(\"building_id\",\"date\",\"slot\")\n",
    "                 .agg(F.sum(\"energy\").alias(\"energy_6h_total\")))\n",
    "    print(f\"\\n--- (b) 6h energy by building — batch {epochId} ---\")\n",
    "    six.orderBy(\"date\",\"slot\",\"building_id\").show(20, truncate=False)\n",
    "\n",
    "    if WRITE_PARQUET_B:\n",
    "        (six\n",
    "          .write\n",
    "          .mode(\"append\")\n",
    "          .partitionBy(\"building_id\",\"date\",\"slot\")\n",
    "          .parquet(OUT_B))\n",
    "\n",
    "    # --- (c) daily energy by site (every second batch ~14s) ---\n",
    "    if epochId % 2 == 0:\n",
    "        daily = (preds.groupBy(\"site_id\",\"date\")\n",
    "                       .agg(F.sum(\"energy\").alias(\"daily_energy\")))\n",
    "        print(f\"\\n--- (c) daily energy by site — batch {epochId} ---\")\n",
    "        daily.orderBy(\"date\",\"site_id\").show(50, truncate=False)\n",
    "\n",
    "        if WRITE_PARQUET_C:\n",
    "            (daily\n",
    "              .write\n",
    "              .mode(\"append\")\n",
    "              .partitionBy(\"site_id\",\"date\")\n",
    "              .parquet(OUT_C))\n",
    "\n",
    "    preds.unpersist()\n",
    "\n",
    "# Start ONE streaming query with a 7s trigger; (c) runs every 2nd batch => ~14s\n",
    "one_q = (\n",
    "    weather_6h\n",
    "    .writeStream\n",
    "    .queryName(\"one_stream_three_tasks\")\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", os.path.join(CKPT_ONE, \"one_stream_three_tasks_v11\"))\n",
    "    .trigger(processingTime=\"7 seconds\")\n",
    "    .foreachBatch(process_all)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Optional: small peek after a couple of batches\n",
    "time.sleep(10)\n",
    "print(one_q.lastProgress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_q.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tSave the data from 6 to Parquet files as streams. (Hint: Parquet files support streaming writing/reading. The file keeps updating while new batches arrive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6a running — writing predictions to /tmp/stream_jobs/out/predictions_a_v02\n",
      "[6a/batch 0] no new rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ===== 6a) PREDICTIONS → PARQUET (STREAM) =====\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")       # down from 200\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"64m\")\n",
    "spark.conf.set(\"spark.sql.streaming.fileSource.logSegmentBytes\", \"1048576\")  # 1MB segments\n",
    "\n",
    "\n",
    "OUT_A  = \"/tmp/stream_jobs/out/predictions_a_v02\"   # use a new path if you changed partitioning\n",
    "CKPT_A = os.path.join(CHECKPOINT_DIR, \"predictions_a_ckpt_v02\")\n",
    "\n",
    "CLAMP_CAP = 5000.0\n",
    "PRINT_SAMPLE_ROWS = 10\n",
    "\n",
    "def write_predictions(batch_df, epochId: int):\n",
    "    if batch_df.head(1) == []:\n",
    "        print(f\"[6a/batch {epochId}] no new rows\")\n",
    "        return\n",
    "\n",
    "    # 1) site→building expansion for current batch\n",
    "    wb = join_weather_buildings(batch_df)\n",
    "\n",
    "    # 2) add lag/rolling features from historical energy (left join)\n",
    "    wb_lag = (\n",
    "        wb.join(\n",
    "            F.broadcast(\n",
    "                energy_enriched.select(\n",
    "                    \"building_id\",\"date\",\"slot\",\n",
    "                    \"lag_1\",\"lag_4\",\"lag_28\",\"roll_mean_4\",\"roll_std_4\"\n",
    "                )\n",
    "            ),\n",
    "            on=[\"building_id\",\"date\",\"slot\"], how=\"left\"\n",
    "        )\n",
    "        .fillna({\"lag_1\":0.0,\"lag_4\":0.0,\"lag_28\":0.0,\"roll_mean_4\":0.0,\"roll_std_4\":0.0})\n",
    "    )\n",
    "\n",
    "    # 3) shape features for the model\n",
    "    shaped = drop_model_outputs_if_exist(shape_for_model_from_joined(wb_lag))\n",
    "\n",
    "    # 4) predict\n",
    "    preds = (\n",
    "        model.transform(shaped)\n",
    "        .select(\"building_id\",\"site_id\",\"date\",\"slot\",\n",
    "                F.col(\"prediction\").alias(\"energy_raw\"))\n",
    "        .withColumn(\"energy\", F.greatest(F.lit(0.0),\n",
    "                              F.least(F.col(\"energy_raw\"), F.lit(CLAMP_CAP))))\n",
    "        .drop(\"energy_raw\")\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "\n",
    "    # 5) write to parquet (INSIDE this function)\n",
    "    (preds\n",
    "        .coalesce(2)                # fewer, larger files\n",
    "        .write.mode(\"append\")\n",
    "        .partitionBy(\"date\")        # simpler partitioning to reduce file listing\n",
    "        .parquet(OUT_A)\n",
    "    )\n",
    "\n",
    "    preds.unpersist()\n",
    "\n",
    "# Start the streaming query (from your weather_6h upstream)\n",
    "predictions_q = (\n",
    "    weather_6h\n",
    "      .writeStream\n",
    "      .outputMode(\"update\")\n",
    "      .queryName(\"predictions_to_parquet_v2\")\n",
    "      .foreachBatch(write_predictions)\n",
    "      .trigger(processingTime=\"7 seconds\")\n",
    "      .option(\"checkpointLocation\", CKPT_A)\n",
    "      .start()\n",
    ")\n",
    "\n",
    "print(\"✅ 6a running — writing predictions to\", OUT_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6b running → /tmp/stream_jobs/out/energy_6h_by_building_ver02\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"64m\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"site_id\",     IntegerType(), True),\n",
    "    StructField(\"date\",        StringType(),  True),\n",
    "    StructField(\"slot\",        IntegerType(), True),\n",
    "    StructField(\"energy\",      DoubleType(),  True),\n",
    "])\n",
    "\n",
    "IN_A  = \"/tmp/stream_jobs/out/predictions_a_v02\"\n",
    "OUT_B = \"/tmp/stream_jobs/out/energy_6h_by_building_ver02\"\n",
    "CKPT_B = os.path.join(CHECKPOINT_DIR, \"energy_6h_by_building_ckpt_v02\")  # NEW\n",
    "\n",
    "energy_source = (\n",
    "    spark.readStream\n",
    "         .schema(schema)\n",
    "         .option(\"maxFilesPerTrigger\", 20)\n",
    "         .parquet(IN_A)\n",
    ")\n",
    "\n",
    "building_6h_stream = (\n",
    "    energy_source\n",
    "    .groupBy(\"building_id\", \"date\", \"slot\")\n",
    "    .agg(F.sum(\"energy\").alias(\"energy_6h_total\"))\n",
    ")\n",
    "\n",
    "def write_b(df, epochId: int):\n",
    "    rows = df.count()\n",
    "    if rows == 0:\n",
    "        return\n",
    "    (df.coalesce(2)\n",
    "       .write.mode(\"append\")\n",
    "       .partitionBy(\"date\",\"slot\",\"building_id\")\n",
    "       .parquet(OUT_B))\n",
    "\n",
    "b_parquet_q = (\n",
    "    building_6h_stream.writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .queryName(\"energy_6h_from_v4_b\")\n",
    "    .foreachBatch(write_b)\n",
    "    .option(\"checkpointLocation\", CKPT_B)\n",
    "    .trigger(processingTime=\"7 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"✅ 6b running →\", OUT_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 6c running — writing daily-by-site to /tmp/stream_jobs/out/energy_daily_by_site_ver02\n"
     ]
    }
   ],
   "source": [
    "# ===== 6c) STREAM DAILY ENERGY TOTALS BY SITE TO PARQUET (with logging) =====\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# Reuse the SAME energy_source used by 6b (do NOT recreate with a different path).\n",
    "# energy_source has schema: building_id, site_id, date, slot, energy\n",
    "\n",
    "OUT_C  = \"/tmp/stream_jobs/out/energy_daily_by_site_ver02\"\n",
    "CKPT_C = os.path.join(CHECKPOINT_DIR, \"energy_daily_by_site_ckpt_ver02\")  # <- NEW checkpoint\n",
    "\n",
    "site_daily_stream = (\n",
    "    energy_source\n",
    "    .groupBy(\"site_id\", \"date\")\n",
    "    .agg(F.sum(\"energy\").alias(\"daily_energy\"))\n",
    ")\n",
    "\n",
    "def write_c(df, epochId: int):\n",
    "    # cheap emptiness check for streaming batches\n",
    "    if not df.take(1):\n",
    "        print(f\"[6c/batch {epochId}] no new rows\")\n",
    "        return\n",
    "\n",
    "    # write out\n",
    "    (df.coalesce(2)\n",
    "       .write\n",
    "       .mode(\"append\")\n",
    "       .partitionBy(\"date\")             # daily partition\n",
    "       .parquet(OUT_C))\n",
    "\n",
    "# stop any old 6c with same name\n",
    "for q in spark.streams.active:\n",
    "    if q.name == \"energy_daily_by_site_v3\":\n",
    "        print(\"stopping old\", q.name)\n",
    "        q.stop()\n",
    "\n",
    "c_parquet_q = (\n",
    "    site_daily_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")               # fine with foreachBatch\n",
    "    .queryName(\"energy_daily_by_site_v3\")\n",
    "    .foreachBatch(write_c)\n",
    "    .option(\"checkpointLocation\", CKPT_C)\n",
    "    .trigger(processingTime=\"14 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"✅ 6c running — writing daily-by-site to\", OUT_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tRead the parquet files from task 7 as data streams and send them to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "# ---------- Paths (adjust if you moved outputs off /tmp) ----------\n",
    "IN_A = \"/tmp/stream_jobs/out/predictions_a_v02\"\n",
    "IN_B = \"/tmp/stream_jobs/out/energy_6h_by_building_ver02\"\n",
    "IN_C = \"/tmp/stream_jobs/out/energy_daily_by_site_ver02\"\n",
    "\n",
    "CHECKPOINT_BASE = \"/home/student/work/ass2b/ckpts_task02\"\n",
    "os.makedirs(CHECKPOINT_BASE, exist_ok=True)\n",
    "\n",
    "KAFKA_BOOTSTRAP = \"kafka:9092\"   # your docker-compose host:port\n",
    "\n",
    "# ---------- Schemas ----------\n",
    "# A) predictions from 6a\n",
    "pred_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"site_id\",     IntegerType(), True),\n",
    "    StructField(\"date\",        StringType(),  True),   # was written as string in your code\n",
    "    StructField(\"slot\",        IntegerType(), True),\n",
    "    StructField(\"energy\",      DoubleType(),  True),\n",
    "])\n",
    "\n",
    "# B) 6-hour totals by building (from 6b)\n",
    "b_schema = StructType([\n",
    "    StructField(\"building_id\",    IntegerType(), True),\n",
    "    StructField(\"date\",           StringType(),  True),\n",
    "    StructField(\"slot\",           IntegerType(), True),\n",
    "    StructField(\"energy_6h_total\",DoubleType(),  True),\n",
    "])\n",
    "\n",
    "# C) daily totals by site (from 6c)\n",
    "c_schema = StructType([\n",
    "    StructField(\"site_id\",      IntegerType(), True),\n",
    "    StructField(\"date\",         StringType(),  True),\n",
    "    StructField(\"daily_energy\", DoubleType(),  True),\n",
    "])\n",
    "\n",
    "# ---------- Utility: build Kafka (key, value) columns ----------\n",
    "def to_kafka_cols(df, key_cols, value_cols=None):\n",
    "    \"\"\"Return df with 'key' and 'value' as STRING, ready for Kafka sink.\"\"\"\n",
    "    if value_cols is None:\n",
    "        value_cols = df.columns\n",
    "    key_col = F.concat_ws(\":\", *[F.col(c).cast(\"string\") for c in key_cols]).alias(\"key\")\n",
    "    val_col = F.to_json(F.struct(*[F.col(c) for c in value_cols])).alias(\"value\")\n",
    "    return df.select(key_col, val_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ task7-A: streaming Parquet → Kafka topic 'predictions_a'\n"
     ]
    }
   ],
   "source": [
    "predictions_stream = (\n",
    "    spark.readStream.schema(pred_schema)\n",
    "         .option(\"maxFilesPerTrigger\", 50)   # tune 25–200\n",
    "         .option(\"latestFirst\", \"true\")\n",
    "         .option(\"failOnDataLoss\", \"false\")\n",
    "         .parquet(IN_A)\n",
    ")\n",
    "\n",
    "predictions_kafka = to_kafka_cols(\n",
    "    predictions_stream,\n",
    "    key_cols=[\"building_id\", \"site_id\", \"date\", \"slot\"],     # consistent partitioning key\n",
    "    value_cols=[\"building_id\",\"site_id\",\"date\",\"slot\",\"energy\"]\n",
    ")\n",
    "\n",
    "q_pred = (\n",
    "    predictions_kafka\n",
    "      .select(F.col(\"key\").cast(\"binary\").alias(\"key\"),\n",
    "              F.col(\"value\").cast(\"binary\").alias(\"value\"))\n",
    "      .writeStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "      .option(\"topic\", \"predictions_a\")\n",
    "      .option(\"checkpointLocation\", os.path.join(CHECKPOINT_BASE, \"predictions_a\"))\n",
    "      .outputMode(\"append\")\n",
    "      .start()\n",
    ")\n",
    "\n",
    "print(\"▶️ task7-A: streaming Parquet → Kafka topic 'predictions_a'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ task7-B: streaming Parquet → Kafka topic 'energy_6h_by_building'\n"
     ]
    }
   ],
   "source": [
    "sixh_stream = (\n",
    "    spark.readStream.schema(b_schema)\n",
    "         .option(\"maxFilesPerTrigger\", 50)\n",
    "         .option(\"latestFirst\", \"true\")\n",
    "         .option(\"failOnDataLoss\", \"false\")\n",
    "         .parquet(IN_B)\n",
    ")\n",
    "\n",
    "sixh_kafka = to_kafka_cols(\n",
    "    sixh_stream,\n",
    "    key_cols=[\"building_id\", \"date\", \"slot\"],\n",
    "    value_cols=[\"building_id\",\"date\",\"slot\",\"energy_6h_total\"]\n",
    ")\n",
    "\n",
    "q_b = (\n",
    "    sixh_kafka\n",
    "      .select(F.col(\"key\").cast(\"binary\").alias(\"key\"),\n",
    "              F.col(\"value\").cast(\"binary\").alias(\"value\"))\n",
    "      .writeStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "      .option(\"topic\", \"energy_6h_by_building\")\n",
    "      .option(\"checkpointLocation\", os.path.join(CHECKPOINT_BASE, \"energy_6h_by_building\"))\n",
    "      .outputMode(\"append\")\n",
    "      .start()\n",
    ")\n",
    "\n",
    "print(\"▶️ task7-B: streaming Parquet → Kafka topic 'energy_6h_by_building'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ task7-C: streaming Parquet → Kafka topic 'energy_daily_by_site'\n"
     ]
    }
   ],
   "source": [
    "daily_stream = (\n",
    "    spark.readStream.schema(c_schema)\n",
    "         .option(\"maxFilesPerTrigger\", 50)\n",
    "         .option(\"latestFirst\", \"true\")\n",
    "         .option(\"failOnDataLoss\", \"false\")\n",
    "         .parquet(IN_C)\n",
    ")\n",
    "\n",
    "daily_kafka = to_kafka_cols(\n",
    "    daily_stream,\n",
    "    key_cols=[\"site_id\", \"date\"],\n",
    "    value_cols=[\"site_id\",\"date\",\"daily_energy\"]\n",
    ")\n",
    "\n",
    "q_c = (\n",
    "    daily_kafka\n",
    "      .select(F.col(\"key\").cast(\"binary\").alias(\"key\"),\n",
    "              F.col(\"value\").cast(\"binary\").alias(\"value\"))\n",
    "      .writeStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "      .option(\"topic\", \"energy_daily_by_site\")\n",
    "      .option(\"checkpointLocation\", os.path.join(CHECKPOINT_BASE, \"energy_daily_by_site\"))\n",
    "      .outputMode(\"append\")\n",
    "      .start()\n",
    ")\n",
    "\n",
    "print(\"▶️ task7-C: streaming Parquet → Kafka topic 'energy_daily_by_site'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
